# -*- coding: utf-8 -*-
"""fish_trading_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SPiGBP5ayJjyVqYwcZbboVBGWXZBFaBV
"""

!pip install pydrive

"""## Authenticate Colab and Get Fish Data"""

# Import PyDrive and associated libraries.
# This only needs to be done once per notebook.
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Download a file based on its file ID.
#
# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz

#link: https://drive.google.com/file/d/1_MIr11KGN5tJHsxh-kaB09mMuri8yqNz/view?usp=sharing

file_id = "1_5AdAjK90Clz6W6aFzpb-PEenNpNrvk4"
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('fish_data2.csv')

!pip install category_encoders

"""## Import relevant libraries"""

import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from numpy import mean,std
import category_encoders as ce
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.ensemble import BaggingRegressor,AdaBoostRegressor, VotingRegressor
from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score
from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler


# RMSE: lower values are good, while high values are worst.
# R2; Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)
# explained_variance_score: Best possible score is 1.0, lower values are worse.

warnings.filterwarnings('ignore')
pd.set_option('display.max_rows', None)

"""## Import Data"""

data = pd.read_csv("fish_data2.csv")

data.head()

# Check data shape
data.shape

#Check data information

data.info()

# Check duplicated rows

duplicate_rows_df = data[data.duplicated()]
print("number of duplicate rows: ", duplicate_rows_df.shape)   # Duplicated rows are not detected

# print first first rows
data.head()

# print last five rows
data.tail()

data['Year'].unique()

"""## Data Cleaning and Preparation"""

#Rename country header and delete column(s)

df = data.set_axis(['country', 'year', 'commodity', 'flow', 'trade', 'weight', 'quantity_name', 'quantity'], axis=1)
df.drop('quantity_name', axis=1, inplace=True)

df.head()

# convert values in 'commodity' column to lowercases, then create 'new_commodity' column by filtering keywords in 'commodity' column

df['commodity'] = df['commodity'].str.lower()
df.insert(loc = 3,
          column = 'new_commodity',
          value = '')

df.head()

# Function for fish grouping

def fish_grouping(q):

    # The groupings of fish species is done below
    orna_keyword = 'ornamental'
    freshwater_keyword = ['nile perch','salmonidae','trout','tilapias','catfish','carp']
    groundfish_keyword = ['cod','coalfish','dogfish','pollock','flatfish','plaice','hake','halibut','sole','blue whitings',
                   'haddock','turbots','toothfish']
    pelagicfish_keyword = ['eel','swordfish','herring','anchovy','salmon','sardine','seabass','mackerel','tuna','seabream']

    shellfish_keyword = ['crustaceans','molluscs','oysters','shrimps','prawns']


    for i in range(0, len(df)):
        if orna_keyword in df['commodity'][i]:
            df['new_commodity'][i] = 'ornamentals'

        for j in freshwater_keyword:
            if j in df['commodity'][i]:
                df['new_commodity'][i] = 'freshwater_fish'

        for k in groundfish_keyword:
            if k in df['commodity'][i]:
                df['new_commodity'][i] = 'ground_fish'

        for x in pelagicfish_keyword:
            if x in df['commodity'][i]:
                df['new_commodity'][i] = 'pelagic_fish'

        for z in shellfish_keyword:
            if z in df['commodity'][i]:
                df['new_commodity'][i] = 'shell_fish'

    return df

fish_grp = fish_grouping(df)

fish_grp.head()

# Drop the 'commodity' column
fish_grp.drop('commodity', axis=1, inplace=True)

fish_grp.head()

# Add the region column

fish_grp.insert(loc = 1,
          column = 'region',
          value = '')

# Function to group countries into regions.

def region_grouping(q):

    # The groupings of fish species is done below
    southern_africa = ['Angola','Botswana','Lesotho','Mozambique','Namibia',
                               'South Africa','Swaziland','Zambia','Zimbabwe']

    eastern_africa = ['Burundi','Comoros','Eritrea','Ethiopia','Kenya','Madagascar','Malawi',
                      'Mauritius','Rwanda','Seychelles','Somalia','South Sudan','Tanzania','Uganda']

    #dem indicates Dem. Rep. of Congo, and equatorial means Equatorial-Guinea
    central_africa = ['Cameroon','Central African Republic','Chad','Congo',
                      'Dem', 'Equatorial','Gabon','Sao Tome and Principe']

    #cote denotes Cote d'Ivoire, and bissau represents Guinea-Bissau
    western_africa = ['Benin','Bukina faso','Cabo Verde', 'Cote','Gambia','Ghana','Guinea','Bissau','Liberia',
                      'Mali','Mauritania','Niger','Nigeria','Senegal','Sierra Leone','Togo']


    for i in range(0, len(fish_grp)):
        for j in southern_africa:
            if j in df['country'][i]:
                df['region'][i] = 'southern_africa'

        for j in eastern_africa:
            if j in df['country'][i]:
                df['region'][i] = 'eastern_africa'

        for j in central_africa:
            if j in df['country'][i]:
                df['region'][i] = 'central_africa'

        for j in western_africa:
            if j in df['country'][i]:
                df['region'][i] = 'western_africa'


    return fish_grp

region_grp = region_grouping(fish_grp)
region_grp.head()

# Checking unique values in the 'new commodity' column
region_grp['new_commodity'].unique()

region_grp['new_commodity'] = region_grp['new_commodity'].replace('', np.nan)

region_grp['new_commodity'] = region_grp['new_commodity'].fillna(region_grp['new_commodity'].mode()[0])

# Checking unique values in the 'new commodity' column
region_grp['new_commodity'].unique()

# Checking the countries in the data

region_grp['country'].unique()

# Checking the unique values in the flow column
region_grp.flow.unique()

# Checking the number of countries in the data

fish_grp['country'].nunique()

region_grp['region'].unique()

region_grp['region'] = region_grp['region'].replace('', np.nan)

region_grp['region'] = region_grp['region'].fillna(region_grp['region'].mode()[0])

# Checking unique values in the 'new commodity' column
region_grp['region'].unique()

# Check missing values
region_grp.isnull().sum()

# weight and quantity columns contains missing values. Replace missing values using mean of each column
region_grp[['weight','quantity']] = region_grp[['weight','quantity']].fillna(region_grp[['weight','quantity']].mean())

# Re-check missing values in weight and quantity columns
region_grp.isnull().sum()

region_grp.count()      # Used to count the number of rows

# Investigate statistical summary of the data

region_grp[['trade', 'quantity']].describe().T

"""## Exploratory Data Analysis EDA"""

region_grp.head()

#select relevant columns for groupby analysis

rel_col = region_grp[['country', 'region', 'year', 'flow', 'new_commodity', 'trade', 'quantity']]
rel_col.head()

"""## 1. GroupBy Functions"""

# Investigating the number of flow (either import, explore, or re-export) in each country per year.

num_flow = rel_col.groupby(['country', 'year', 'new_commodity'])['flow'].agg(["count"])
num_flow

# Investigating the trade summary in each country per year.

trade_summmary = rel_col.groupby(['country', 'year'])['trade'].agg(
                ["sum", "mean","max", "min"])
trade_summmary

# Investigating the trade summary in each country per year per flow and commodity.

trade_summmary2 = rel_col.groupby(['country', 'year', 'new_commodity', 'flow'])['trade'].agg(
                ["sum", "mean","max", "min"])
trade_summmary2

# Investigating the quantity summary in each country per year per flow and commodity.

quantity_summmary = rel_col.groupby(['country', 'year', 'new_commodity', 'flow'])['quantity'].agg(
                ["sum", "mean","max", "min"])
quantity_summmary

# Investigating the quantity summary in each country per year per commodity.

quantity_summmary = rel_col.groupby(['country', 'year', 'new_commodity',])['quantity'].agg(
                ["sum", "mean","max", "min"])
quantity_summmary

# Investigating trade and quantity summary in each country per year per flow and commodity.

quantity_summmary = rel_col.groupby(['country', 'year', 'new_commodity', 'flow'])[['trade','quantity']].agg(
                ["sum", "mean","max", "min"])
quantity_summmary

# Region import-export distribution for commodity per country per year

region_summary = rel_col.groupby(['region','country', 'year', 'new_commodity'])['flow'].agg(
                ["count"])
region_summary

# Region import-export distribution from 2000-2023 per country per commodity

region_summary2 = rel_col.groupby(['region','country', 'new_commodity'])['flow'].agg(
                ["count"])
region_summary2

# Region trade and quantity distribution from 2000-2023 per country per commodity

region_summary3 = rel_col.groupby(['region','country', 'new_commodity'])[['trade', 'quantity']].agg(
                ["sum", "mean", "std", "max", "min"])
#region_summary3 = rel_col.groupby(['region','country', 'new_commodity'])['trade', 'quantity'].sum()

region_summary3

# Separate numerical and categorical variables for easy analysis

cat_cols=rel_col.select_dtypes(include=['object']).columns
num_cols = rel_col.select_dtypes(include=np.number).columns.tolist()
print("Categorical Variables:")
print(cat_cols)
print("Numerical Variables:")
print(num_cols)

"""## EDA Univariate Analysis
Analyzing/visualizing the dataset by taking one variable at a time. <br>

Univariate analysis can be done for both Categorical and Numerical variables.

Categorical variables can be visualized using a Count plot, Bar Chart, Pie Plot, etc.

Numerical Variables can be visualized using Histogram, Box Plot, Density Plot, etc.
"""

fig, axes = plt.subplots(2,2, figsize = (15, 12))
plt.subplots_adjust(hspace = 0.5)
fig.suptitle('Bar plot for all categorical variables in the dataset')
sns.set_palette("Set2")
sns.countplot(ax = axes[0, 0], x = 'year', data = rel_col,
              order = rel_col['year'].value_counts().index,width=0.3);
sns.countplot(ax = axes[0, 1], x = 'region', data = rel_col,
              order = rel_col['region'].value_counts().index,width=0.3);
sns.countplot(ax = axes[1, 0], x = 'flow', data = rel_col,
              order = rel_col['flow'].value_counts().index,width=0.3);
sns.countplot(ax = axes[1, 1], x = 'new_commodity', data = rel_col,
              order = rel_col['new_commodity'].value_counts().index,width=0.3);
axes[0][0].tick_params(labelrotation=45);
axes[0][1].tick_params(labelrotation=90);
#axes[2][2].tick_params(labelrotation=90);

sns.displot(rel_col, x="new_commodity", height=4.5, aspect=5.0)
plt.show()

"""## EDA Bivariate Analysis

Bivariate Analysis helps to understand how variables are related to each other and the relationship between dependent and independent variables present in the dataset.

For Numerical variables, Pair plots and Scatter plots are widely been used to do Bivariate Analysis.

A Stacked bar chart can be used for categorical variables if the output variable is a classifier. Bar plots can be used if the output variable is continuous.
"""

rel_col.head()

rel_col['flow'].unique()

rel_col['region'].unique()

rel_col['new_commodity'].unique()

rel_col['year'].unique()

#count plot along y axis


sns.set(rc={'figure.figsize':(10,15)}, font_scale=0.8, style='whitegrid')

#sns.set(style="darkgrid")

sns.boxplot(x='trade',
            y='country',
            data=rel_col,
            showmeans=True,
            meanprops={"marker": "+",
                       "markeredgecolor": "blue",
                       "markersize": "10"})
plt.ylabel("Country", size=10)
plt.xlabel("Trade", size=10)



# Show the plot
plt.show()

"""## Mutivariate Analysis for Fish Trade

Commodity vs Region
"""

# Plotting total trade for each commodity between 2000-2023 for all regions.

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col, col='new_commodity', hue='region', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

# Save the figure
#g.savefig("facet_plot.png", dpi=1000, bbox_inches="tight")

# Download to your computer
#from google.colab import files
#files.download("facet_plot.png")

"""Region vs Commodity"""

# Plotting total trade for each region between 2000-2023 for all species

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col, col='region', hue='new_commodity', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

"""Region vs Commodity-Import"""

# Subset for Import
df_import = rel_col[rel_col["flow"] == "Import"]

sns.set_style("whitegrid", {'axes.grid': True})
g = sns.FacetGrid(df_import, col='region', hue='new_commodity',
                  aspect=1, height=3.0, despine=True)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name} - Import")
g.add_legend()
g.tight_layout()
plt.show()

# Download plot
from google.colab import files

# Save figure with dpi
g.savefig("trade_specie_region_flow_import.png", dpi=2000, bbox_inches="tight")

# Download file
files.download("trade_specie_region_flow_import.png")

"""Region vs Commodity-Export"""

# Subset for Export
df_export = rel_col[rel_col["flow"] == "Export"]

sns.set_style("whitegrid", {'axes.grid': True})
g = sns.FacetGrid(df_export, col='region', hue='new_commodity',
                  aspect=1, height=3.0, despine=True)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name} - Export")
g.add_legend()
g.tight_layout()
plt.show()

# Download plot
from google.colab import files

# Save figure with dpi
g.savefig("trade_specie_region_flow_export.png", dpi=2000, bbox_inches="tight")

# Download file
files.download("trade_specie_region_flow_export.png")

"""Region vs Commodity-Re-Export"""

# Subset for Re-export
df_reexport = rel_col[rel_col["flow"] == "Re-Export"]

sns.set_style("whitegrid", {'axes.grid': True})
g = sns.FacetGrid(df_reexport, col='region', hue='new_commodity',
                  aspect=1, height=3.0, despine=True)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name} - Re-export")
g.add_legend()
g.tight_layout()
plt.show()

# Download plot
from google.colab import files

# Save figure with dpi
g.savefig("trade_specie_region_flow_re_export.png", dpi=2000, bbox_inches="tight")

# Download file
files.download("trade_specie_region_flow_re_export.png")

"""Region vs Flow"""

# Plotting total trade for each region between 2000-2023 for all flows

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col, col='region', hue='flow', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

"""Flow vs Region"""

# Plotting total trade for each flow between 2000-2023 for all regions

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col, col='flow', hue='region', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

"""Commodity vs Flow"""

# Plotting total trade for each commodity between 2000-2023 for all flows

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col, col='new_commodity', hue='flow', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()


# Download plot
from google.colab import files
#files.download("trade_specie_flow_SSA.png", dpi)

# Save figure with dpi
g.savefig("trade_specie_flow_SSA.png", dpi=2000, bbox_inches="tight")

# Download file
files.download("trade_specie_flow_SSA.png")

"""Flow vs Commodity"""

# Plotting total trade for each flow between 2000-2023 for all commodities

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col, col='flow', hue='new_commodity', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'trade', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

"""## Multivariate Analysis for Fish Quantity"""

rel_col2 = rel_col.copy()
rel_col2.head()

rel_col2.head()

"""Commodity vs Region"""

# Plotting total quantity for each commodity between 2000-2023 for all regions.

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col2, col='new_commodity', hue='region', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

"""Region vs Commodity"""

# Plotting total trade for each commodity between 2000-2023 for all regions.

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col2, col='region', hue='new_commodity', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

"""Region vs Commodity-Import"""

# Subset for Import
df_import = rel_col2[rel_col2["flow"] == "Import"]

sns.set_style("whitegrid", {'axes.grid': True})
g = sns.FacetGrid(df_import, col='region', hue='new_commodity',
                  aspect=1, height=3.0, despine=True)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name} - Import")
g.add_legend()
g.tight_layout()
plt.show()

# Download plot
from google.colab import files

# Save figure with dpi
g.savefig("qty_specie_region_flow_import.png", dpi=2000, bbox_inches="tight")

# Download file
files.download("qty_specie_region_flow_import.png")

"""Region vs Commodity-Export"""

# Subset for Import
df_import = rel_col2[rel_col2["flow"] == "Export"]

sns.set_style("whitegrid", {'axes.grid': True})
g = sns.FacetGrid(df_import, col='region', hue='new_commodity',
                  aspect=1, height=3.0, despine=True)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name} - Emport")
g.add_legend()
g.tight_layout()
plt.show()

# Download plot
from google.colab import files

# Save figure with dpi
g.savefig("qty_specie_region_flow_export.png", dpi=2000, bbox_inches="tight")

# Download file
files.download("qty_specie_region_flow_export.png")

"""Region vs Commodity-Re-Export"""

# Subset for Import
df_import = rel_col2[rel_col2["flow"] == "Re-Export"]

sns.set_style("whitegrid", {'axes.grid': True})
g = sns.FacetGrid(df_import, col='region', hue='new_commodity',
                  aspect=1, height=3.0, despine=True)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name} - Re-Export")
g.add_legend()
g.tight_layout()
plt.show()

# Download plot
from google.colab import files

# Save figure with dpi
g.savefig("qty_specie_region_flow_re_export.png", dpi=2000, bbox_inches="tight")

# Download file
files.download("qty_specie_region_flow_re_export.png")

"""Region vs Flow"""

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col2, col='region', hue='flow', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

"""Flow vs Region"""

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col2, col='flow', hue='region', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

"""Commodity vs Flow"""

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col2, col='new_commodity', hue='flow', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

# Download plot
from google.colab import files
#files.download("trade_specie_flow_SSA.png", dpi)

# Save figure with dpi
g.savefig("qty_specie_flow_SSA.png", dpi=2000, bbox_inches="tight")

# Download file
files.download("qty_specie_flow_SSA.png")

"""Flow vs Commodity"""

sns.set_style("whitegrid", {'axes.grid' : True})
g = sns.FacetGrid(rel_col2, col='flow', hue='new_commodity', aspect=1, height=3.0, despine=True,)
g.map_dataframe(sns.lineplot, 'year', 'quantity_var', ci=None)
g.set_titles(col_template="{col_name}")
g.add_legend()
g.tight_layout()

plt.figure(figsize=(5, 3))
dat = rel_col[['year', 'trade', 'quantity']]
sns.heatmap((dat).corr(), annot = True, vmin = -1, vmax = 1)
plt.show()

rel_col.head()

"""# 2. Explore Pandas Profiling"""

pip install ydata-profiling

from ydata_profiling import ProfileReport

profile = ProfileReport(rel_col, title="Profiling Report", minimal=True)
#profile.to_file(output_file='output.html')
profile

"""## Machine Learning Model Development
Implemented on HPC due to long computation time for hyperparameter optimization
"""

new_df = rel_col.drop(['country'],axis=1)
new_df.head()

# Using One-Hot encoding in category_encoder package to transform region and new_commodity columns to numerical variables.

encoder = ce.OneHotEncoder(cols=['region','flow','new_commodity'], use_cat_names=True)
new_df2 = encoder.fit_transform(new_df)
new_df2.head()

#Separate feature vector from label to check class distribution
X = new_df2.drop(['trade'], axis=1)
y = new_df2[['trade']]

## We drop replace the "trade" column with "quantity" to perform fish quantity prediction

# Split the dataset into 70% Training set and 30% Test set-------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

# Performing feature normalization or standardization-----------------------------------
# The range of values for the attributes are almost of the same range. Standardization will improve the data
# variability and mean.
scale = StandardScaler() # The robust scaler standardize the data and also deals with outliers in the data
X_train = scale.fit_transform(X_train)
X_test = scale.transform(X_test)

"""## Hyperparameter Tuning using random search"""

print('\n')
# MODEL DEVELOPMENT BEGINS
print('# MODEL DEVELOPMENT BEGIN')
# Cross validation of 10 folds and 5 runs
cv_method = RepeatedKFold(n_splits=10, n_repeats=5, random_state=42)

# Hyperparameter Optimization using RandomSearch and CrossValidation to get the best model hyperparamters
# Linear regression and ridge regression
LR = LinearRegression() #'loss': ['linear', 'square']}


# kNN regression
nearest_neighbour = KNeighborsRegressor()
# Create a dictionary of KNN parameters
# K values between 1 and 9 are used to avoid ties and p values of 1 (Manhattan), 2 (Euclidean), and 5 (Minkowski)
param_kNN = {'n_neighbors': [1,3,5,7,9],'p':[1,2,5]} # Distance Metric: Manhattan (p=1), Euclidean (p=2) or
# Minkowski (any p larger than 2). Technically p=1 and p=2 are also Minkowski distances.
# Define the kNN model using RandomSearch and optimize accuracy
kNN_grid = RandomizedSearchCV(nearest_neighbour,param_kNN,scoring='r2',cv=cv_method)
kNN_grid.fit(X_train,y_train)
# Print the best parameter values for KNN
print('kNN Best Parameter values =',kNN_grid.best_params_)
#kNN = KNeighborsClassifier(**kNN_grid.best_params_)
kNN = kNN_grid.best_estimator_


# Support Vector Regression
SVR_model = SVR()
# Create a dictionary of SVM hyperparameters
# Parameter space for rbf kernels
params_SVR = {'kernel':['rbf'],'C':np.linspace(0.1,1.0),
              'gamma':['scale','auto']}

# Using Random Search to explore the best parameter for the a SVM model
SVR_Grid = RandomizedSearchCV(SVR_model, params_SVR,scoring='r2',cv=cv_method)
# Fitting the parameterized model
SVR_Grid.fit(X_train,y_train)
# Print the best parameter values
print('SVC Best Parameter Values:', SVR_Grid.best_params_)
SVR = SVR_Grid.best_estimator_


# Randomforest
RF2 = RandomForestRegressor()
params_RF = {'n_estimators': [10,15,20,25,30],
               'max_depth': [10,15,20,25,30]}
RF_Grid = RandomizedSearchCV(RF2, params_RF,scoring='r2',cv=cv_method)
# Fitting the parameterized model
RF_Grid.fit(X_train,y_train)

# Print the best parameter values
print('RF Best Parameter Values:', RF_Grid.best_params_)
RF = RF_Grid.best_estimator_


#Boosting regressor
Bag = BaggingRegressor()
params_Bag = {'n_estimators':[10,15,20,25,30]}
Bag_Grid = RandomizedSearchCV(Bag, params_Bag,scoring='r2',cv=cv_method)
# Fitting the parameterized model
Bag_Grid.fit(X_train,y_train)

# Print the best parameter values
print('Bagging Best Parameter Values:', Bag_Grid.best_params_)
BAG = Bag_Grid.best_estimator_


#Adaboost regressor
ADA2 = AdaBoostRegressor()
params_ADA = {'n_estimators': [10,15,20,25,30],
               'learning_rate':np.linspace(0.1,1.0)}
ADA_Grid = RandomizedSearchCV(ADA2, params_ADA,scoring='r2',cv=cv_method)
# Fitting the parameterized model
ADA_Grid.fit(X_train,y_train)

# Print the best parameter values
print('RF Best Parameter Values:', ADA_Grid.best_params_)
ADA = ADA_Grid.best_estimator_

print('\n')

"""## Model Training and Evaluation"""

# get a list of models to evaluate
def get_models():
    models = dict()
    models['LR'] = LR
    models['kNN'] = kNN
    models['SVR'] = SVR
    models['RF'] = RF
    models['Bagging'] = BAG
    models['AdaBoost'] = ADA

    return models


# evaluate a given model using cross-validation
def evaluate_model(model, X_train, y_train):
    scores = cross_val_score(model, X_train, y_train, scoring='r2', cv=cv_method, n_jobs=-1)
    return scores


# get the models to evaluate
models = get_models()
# evaluate the models and store results
test_results, names, = list(), list()
print('Mean Accuracy and Std Dev of each model on test set:----------------------------------')
for name, model in models.items():
    scores = evaluate_model(model, X_test, y_test)
    test_results.append(scores)
    names.append(name)
    print('>%s %.5f' % (name, mean(scores)),u"\u00B1", '%.5f' % std(scores))
#test_dict = dict(zip(names, test_results))
#test_dict = pd.DataFrame(test_dict)
print('\n')


print('Mean Accuracy and Std Dev of each model on train set:-----------------------------')
train_results, names, = list(), list()
for name, model in models.items():
    # evaluate the model
    scores = evaluate_model(model, X_train, y_train)
    # store the results
    train_results.append(scores)
    names.append(name)
    # summarize the performance along the way
    print('>%s %.5f' % (name, mean(scores)), u"\u00B1", '%.5f' % std(scores))
#train_dict = dict(zip(names, train_results))
#train_dict = pd.DataFrame(train_dict)
print('\n')


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))
fig.suptitle('Testing and Training Performance of Models', fontsize=12)
fig.tight_layout(pad=4.0)
ax1.boxplot(test_results, labels=names, showfliers=False)
# Add a horizontal grid to the plot, but make it very light in color
# so we can use it for reading data values but not be distracting
ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',
               alpha=0.5)
ax1.set_axisbelow=True
ax1.set_xlabel("Models", fontsize=12)
ax1.set_ylabel("Testing Accuracy", fontsize=12)

ax2.boxplot(train_results, labels=names, showfliers=False)
ax2.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',
               alpha=0.5)

ax2.set_axisbelow=True
ax2.set_xlabel("Models", fontsize=12)
ax2.set_ylabel("Training Accuracy", fontsize=12)
plt.show()


print('\n')
for name, model in models.items():
    #fit the model
    model.fit(X_train, y_train)
    # the product on the test set
    y_pred = model.predict(X_test)
    # evaluate the models
    mse_test = mean_squared_error(y_test, y_pred)
    root_mse_test = np.sqrt(mse_test)
    r2_test = r2_score(y_test, y_pred)
    print('Performance Result of', name, '--------------------------------------------------------------------')
    print(name, 'root mean squared error of test set:', root_mse_test)
    print(name, 'r_squared coefficient of test set:', r2_test)

    y_pred_train = model.predict(X_train)
    # evaluate the models
    mse_train = mean_squared_error(y_train, y_pred_train)
    root_mse_train = np.sqrt(mse_train)
    r2_train = r2_score(y_train, y_pred_train)
    print('Performance Result of', name, '--------------------------------------------------------------------')
    print(name, 'root mean squared error of train set:', root_mse_train)
    print(name, 'r_squared coefficient of train set:', r2_train)
